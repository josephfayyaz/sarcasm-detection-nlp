"""
Inference script for the BESSTIE figurative language project.

This module provides functions to load a fine‑tuned model and generate
predictions for new text data.  Predictions can be run on arbitrary lists
of strings or on a CSV file with a ``text`` column.  The output can be
written to a new CSV file with an additional ``prediction`` column where
``0`` indicates non‑sarcastic / negative and ``1`` indicates sarcastic / positive.
"""

import argparse
import os
from typing import List

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification


def predict_binary(model_name: str, checkpoint_dir: str, texts: List[str]) -> List[int]:
    """Generate binary predictions for a list of texts using a saved model.

    Parameters
    ----------
    model_name : str
        Name of the base model architecture used during training.  Only used
        to initialise the tokenizer; the model weights are loaded from
        ``checkpoint_dir``.
    checkpoint_dir : str
        Directory containing the fine‑tuned model files (generated by
        ``train.py``).
    texts : List[str]
        List of input sentences to classify.

    Returns
    -------
    List[int]
        Predicted labels: ``1`` for positive / sarcastic and ``0`` for
        negative / non‑sarcastic.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(checkpoint_dir)
    model.eval()
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
    with torch.no_grad():
        logits = model(**inputs).logits
    preds = logits.argmax(dim=-1).tolist()
    return preds


def main():
    parser = argparse.ArgumentParser(description="Generate predictions using a fine‑tuned BESSTIE model.")
    parser.add_argument("--model_name", type=str, default="roberta-base", help="Base model architecture used during training")
    parser.add_argument("--checkpoint_dir", type=str, required=True, help="Directory containing the fine‑tuned model")
    parser.add_argument("--input_file", type=str, help="CSV file with a 'text' column to predict labels for")
    parser.add_argument("--output_file", type=str, help="Where to save the CSV with predictions (optional)")
    parser.add_argument("--text", type=str, nargs="*", help="One or more texts to classify (if no input_file is provided)")
    args = parser.parse_args()
    if args.input_file:
        if not os.path.exists(args.input_file):
            raise FileNotFoundError(f"Input file {args.input_file} does not exist")
        df = pd.read_csv(args.input_file)
        if "text" not in df.columns:
            raise ValueError("Input CSV must contain a 'text' column")
        texts = df["text"].astype(str).tolist()
        preds = predict_binary(args.model_name, args.checkpoint_dir, texts)
        df["prediction"] = preds
        output_path = args.output_file or os.path.splitext(args.input_file)[0] + "_predictions.csv"
        df.to_csv(output_path, index=False)
        print(f"Predictions written to {output_path}")
    else:
        if not args.text:
            raise ValueError("Either --input_file or --text must be provided")
        preds = predict_binary(args.model_name, args.checkpoint_dir, args.text)
        for t, p in zip(args.text, preds):
            label = "sarcastic/positive" if p == 1 else "non‑sarcastic/negative"
            print(f"{t} -> {label}")


if __name__ == "__main__":
    main()